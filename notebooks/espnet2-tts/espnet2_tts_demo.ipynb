{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb927a9b-d607-4d1f-b804-24ff482bd25e",
   "metadata": {},
   "source": [
    "# ESPnet2-TTS realtime demonstration\n",
    "\n",
    "This notebook provides a demonstration of the realtime E2E-TTS using ESPnet2-TTS and ParallelWaveGAN repo.\n",
    "\n",
    "- ESPnet2-TTS: https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/tts1\n",
    "- ParallelWaveGAN: https://github.com/kan-bayashi/ParallelWaveGAN\n",
    "\n",
    "Author: Tomoki Hayashi ([@kan-bayashi](https://github.com/kan-bayashi))\n",
    "\n",
    "Original notebooks: https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_tts_realtime_demo.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7811c9d-c7d7-4a60-8875-5ab17afcff8c",
   "metadata": {},
   "source": [
    "# Single speaker model demo\n",
    "## Model Selection\n",
    "\n",
    "Please select model: English, Japanese, and Mandarin are supported.\n",
    "\n",
    "You can try end-to-end text2wav model & combination of text2mel and vocoder.  \n",
    "If you use text2wav model, you do not need to use vocoder (automatically disabled).\n",
    "\n",
    "**Text2wav models**:\n",
    "- VITS\n",
    "\n",
    "**Text2mel models**:\n",
    "- Tacotron2\n",
    "- Transformer-TTS\n",
    "- (Conformer) FastSpeech\n",
    "- (Conformer) FastSpeech2\n",
    "\n",
    "**Vocoders**:\n",
    "- Parallel WaveGAN\n",
    "- Multi-band MelGAN\n",
    "- HiFiGAN\n",
    "- Style MelGAN.\n",
    "\n",
    "\n",
    "> The terms of use follow that of each corpus. We use the following corpora:\n",
    "- `ljspeech_*`: LJSpeech dataset \n",
    "  - https://keithito.com/LJ-Speech-Dataset/\n",
    "- `jsut_*`: JSUT corpus\n",
    "  - https://sites.google.com/site/shinnosuketakamichi/publication/jsut\n",
    "- `jvs_*`: JVS corpus + JSUT corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671469c-34e4-47de-9aab-4f7fe5297cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Choose English model { run: \"auto\" }\n",
    "lang = 'English'\n",
    "tag = 'kan-bayashi/ljspeech_vits' #@param [\"kan-bayashi/ljspeech_tacotron2\", \"kan-bayashi/ljspeech_fastspeech\", \"kan-bayashi/ljspeech_fastspeech2\", \"kan-bayashi/ljspeech_conformer_fastspeech2\", \"kan-bayashi/ljspeech_joint_finetune_conformer_fastspeech2_hifigan\", \"kan-bayashi/ljspeech_joint_train_conformer_fastspeech2_hifigan\", \"kan-bayashi/ljspeech_vits\"] {type:\"string\"}\n",
    "vocoder_tag = \"none\" #@param [\"none\", \"parallel_wavegan/ljspeech_parallel_wavegan.v1\", \"parallel_wavegan/ljspeech_full_band_melgan.v2\", \"parallel_wavegan/ljspeech_multi_band_melgan.v2\", \"parallel_wavegan/ljspeech_hifigan.v1\", \"parallel_wavegan/ljspeech_style_melgan.v1\"] {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb3942-f704-4a57-8a84-e5daac0a8c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Choose Japanese model { run: \"auto\" }\n",
    "lang = 'Japanese'\n",
    "tag = 'kan-bayashi/jsut_full_band_vits_prosody' #@param [\"kan-bayashi/jsut_tacotron2\", \"kan-bayashi/jsut_transformer\", \"kan-bayashi/jsut_fastspeech\", \"kan-bayashi/jsut_fastspeech2\", \"kan-bayashi/jsut_conformer_fastspeech2\", \"kan-bayashi/jsut_conformer_fastspeech2_accent\", \"kan-bayashi/jsut_conformer_fastspeech2_accent_with_pause\", \"kan-bayashi/jsut_vits_accent_with_pause\", \"kan-bayashi/jsut_full_band_vits_accent_with_pause\", \"kan-bayashi/jsut_tacotron2_prosody\", \"kan-bayashi/jsut_transformer_prosody\", \"kan-bayashi/jsut_conformer_fastspeech2_tacotron2_prosody\", \"kan-bayashi/jsut_vits_prosody\", \"kan-bayashi/jsut_full_band_vits_prosody\", \"kan-bayashi/jvs_jvs010_vits_prosody\", \"kan-bayashi/tsukuyomi_full_band_vits_prosody\"] {type:\"string\"}\n",
    "vocoder_tag = 'none' #@param [\"none\", \"parallel_wavegan/jsut_parallel_wavegan.v1\", \"parallel_wavegan/jsut_multi_band_melgan.v2\", \"parallel_wavegan/jsut_style_melgan.v1\", \"parallel_wavegan/jsut_hifigan.v1\"] {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c0ac0-3a2f-40eb-bc99-b68ef023c6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Choose Mandarin model { run: \"auto\" }\n",
    "lang = 'Mandarin'\n",
    "tag = 'kan-bayashi/csmsc_full_band_vits' #@param [\"kan-bayashi/csmsc_tacotron2\", \"kan-bayashi/csmsc_transformer\", \"kan-bayashi/csmsc_fastspeech\", \"kan-bayashi/csmsc_fastspeech2\", \"kan-bayashi/csmsc_conformer_fastspeech2\", \"kan-bayashi/csmsc_vits\", \"kan-bayashi/csmsc_full_band_vits\"] {type: \"string\"}\n",
    "vocoder_tag = \"none\" #@param [\"none\", \"parallel_wavegan/csmsc_parallel_wavegan.v1\", \"parallel_wavegan/csmsc_multi_band_melgan.v2\", \"parallel_wavegan/csmsc_hifigan.v1\", \"parallel_wavegan/csmsc_style_melgan.v1\"] {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf8643-c2b8-46f7-b1e7-4f51a850fed9",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b00b505-526f-4618-9c25-bfce90f1137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from espnet2.bin.tts_inference import Text2Speech\n",
    "from espnet2.utils.types import str_or_none\n",
    "\n",
    "text2speech = Text2Speech.from_pretrained(\n",
    "    model_tag=str_or_none(tag),\n",
    "    vocoder_tag=str_or_none(vocoder_tag),\n",
    "    device=\"cuda\",\n",
    "    # Only for Tacotron 2 & Transformer\n",
    "    threshold=0.5,\n",
    "    # Only for Tacotron 2\n",
    "    minlenratio=0.0,\n",
    "    maxlenratio=10.0,\n",
    "    use_att_constraint=False,\n",
    "    backward_window=1,\n",
    "    forward_window=3,\n",
    "    # Only for FastSpeech & FastSpeech2 & VITS\n",
    "    speed_control_alpha=1.0,\n",
    "    # Only for VITS\n",
    "    noise_scale=0.333,\n",
    "    noise_scale_dur=0.333,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68d2658-2a04-4d26-8e62-ca1dda844ff3",
   "metadata": {},
   "source": [
    "# Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05008f93-c901-4777-96bd-4289f2d56264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# decide the input sentence by yourself\n",
    "print(f\"Input your favorite sentence in {lang}.\")\n",
    "x = input()\n",
    "\n",
    "# synthesis\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    wav = text2speech(x)[\"wav\"]\n",
    "rtf = (time.time() - start) / (len(wav) / text2speech.fs)\n",
    "print(f\"RTF = {rtf:5f}\")\n",
    "\n",
    "# let us listen to generated samples\n",
    "from IPython.display import display, Audio\n",
    "display(Audio(wav.view(-1).cpu().numpy(), rate=text2speech.fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b607b8-8c6b-4bff-9bd4-d33c23c83515",
   "metadata": {},
   "source": [
    "# Multi-speaker Model Demo\n",
    "## Model Selection\n",
    "\n",
    "Now we provide only English multi-speaker pretrained model.\n",
    "\n",
    "> The terms of use follow that of each corpus. We use the following corpora:\n",
    "- `libritts_*`: LibriTTS corpus\n",
    "  - http://www.openslr.org/60\n",
    "- `vctk_*`: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit\n",
    "  - http://www.udialogue.org/download/cstr-vctk-corpus.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d853080-101d-4a4d-b031-3aca6adfdaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title English multi-speaker pretrained model { run: \"auto\" }\n",
    "lang = 'English'\n",
    "tag = 'kan-bayashi/vctk_full_band_multi_spk_vits' #@param [\"kan-bayashi/vctk_gst_tacotron2\", \"kan-bayashi/vctk_gst_transformer\", \"kan-bayashi/vctk_xvector_tacotron2\", \"kan-bayashi/vctk_xvector_transformer\", \"kan-bayashi/vctk_xvector_conformer_fastspeech2\", \"kan-bayashi/vctk_gst+xvector_tacotron2\", \"kan-bayashi/vctk_gst+xvector_transformer\", \"kan-bayashi/vctk_gst+xvector_conformer_fastspeech2\", \"kan-bayashi/vctk_multi_spk_vits\", \"kan-bayashi/vctk_full_band_multi_spk_vits\", \"kan-bayashi/libritts_xvector_transformer\", \"kan-bayashi/libritts_xvector_conformer_fastspeech2\", \"kan-bayashi/libritts_gst+xvector_transformer\", \"kan-bayashi/libritts_gst+xvector_conformer_fastspeech2\", \"kan-bayashi/libritts_xvector_vits\"] {type:\"string\"}\n",
    "vocoder_tag = \"none\" #@param [\"none\", \"parallel_wavegan/vctk_parallel_wavegan.v1.long\", \"parallel_wavegan/vctk_multi_band_melgan.v2\", \"parallel_wavegan/vctk_style_melgan.v1\", \"parallel_wavegan/vctk_hifigan.v1\", \"parallel_wavegan/libritts_parallel_wavegan.v1.long\", \"parallel_wavegan/libritts_multi_band_melgan.v2\", \"parallel_wavegan/libritts_hifigan.v1\", \"parallel_wavegan/libritts_style_melgan.v1\"] {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d8cfdb-c5b3-42ba-b769-4c9da5e65ff2",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60775eeb-2ddd-448c-8bde-b67b9092cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from espnet2.bin.tts_inference import Text2Speech\n",
    "from espnet2.utils.types import str_or_none\n",
    "\n",
    "text2speech = Text2Speech.from_pretrained(\n",
    "    model_tag=str_or_none(tag),\n",
    "    vocoder_tag=str_or_none(vocoder_tag),\n",
    "    device=\"cuda\",\n",
    "    # Only for Tacotron 2 & Transformer\n",
    "    threshold=0.5,\n",
    "    # Only for Tacotron 2\n",
    "    minlenratio=0.0,\n",
    "    maxlenratio=10.0,\n",
    "    use_att_constraint=False,\n",
    "    backward_window=1,\n",
    "    forward_window=3,\n",
    "    # Only for FastSpeech & FastSpeech2 & VITS\n",
    "    speed_control_alpha=1.0,\n",
    "    # Only for VITS\n",
    "    noise_scale=0.333,\n",
    "    noise_scale_dur=0.333,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef3dc4-f710-48ab-8fc1-af43eae373c4",
   "metadata": {},
   "source": [
    "# Speaker selection\n",
    "\n",
    "For multi-speaker model, we need to provide X-vector and/or the reference speech to decide the speaker characteristics.  \n",
    "For X-vector, you can select the speaker from the dumped x-vectors.  \n",
    "For the reference speech, you can use any speech but please make sure the sampling rate is matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81c0415-9921-4d06-9efb-df4dc8a1dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import kaldiio\n",
    "\n",
    "# Get model directory path\n",
    "from espnet_model_zoo.downloader import ModelDownloader\n",
    "d = ModelDownloader()\n",
    "model_dir = os.path.dirname(d.download_and_unpack(tag)[\"train_config\"])\n",
    "\n",
    "# X-vector selection\n",
    "spembs = None\n",
    "if text2speech.use_spembs:\n",
    "    xvector_ark = [p for p in glob.glob(f\"{model_dir}/../../dump/**/spk_xvector.ark\", recursive=True) if \"tr\" in p][0]\n",
    "    xvectors = {k: v for k, v in kaldiio.load_ark(xvector_ark)}\n",
    "    spks = list(xvectors.keys())\n",
    "\n",
    "    # randomly select speaker\n",
    "    random_spk_idx = np.random.randint(0, len(spks))\n",
    "    spk = spks[random_spk_idx]\n",
    "    spembs = xvectors[spk]\n",
    "    print(f\"selected spk: {spk}\")\n",
    "\n",
    "# Speaker ID selection\n",
    "sids = None\n",
    "if text2speech.use_sids:\n",
    "    spk2sid = glob.glob(f\"{model_dir}/../../dump/**/spk2sid\", recursive=True)[0]\n",
    "    with open(spk2sid) as f:\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "    sid2spk = {int(line.split()[1]): line.split()[0] for line in lines}\n",
    "    \n",
    "    # randomly select speaker\n",
    "    sids = np.array(np.random.randint(1, len(sid2spk)))\n",
    "    spk = sid2spk[int(sids)]\n",
    "    print(f\"selected spk: {spk}\")\n",
    "\n",
    "# Reference speech selection for GST\n",
    "speech = None\n",
    "if text2speech.use_speech:\n",
    "    # you can change here to load your own reference speech\n",
    "    # e.g.\n",
    "    # import soundfile as sf\n",
    "    # speech, fs = sf.read(\"/path/to/reference.wav\")\n",
    "    # speech = torch.from_numpy(speech).float()\n",
    "    speech = torch.randn(50000,) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb711c9-7a0b-4047-a36a-88d3e93b494f",
   "metadata": {},
   "source": [
    "# Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff5af2-d2f0-477a-872f-c91841584925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# decide the input sentence by yourself\n",
    "print(f\"Input your favorite sentence in {lang}.\")\n",
    "x = input()\n",
    "\n",
    "# synthesis\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    wav = text2speech(x, speech=speech, spembs=spembs, sids=sids)[\"wav\"]\n",
    "rtf = (time.time() - start) / (len(wav) / text2speech.fs)\n",
    "print(f\"RTF = {rtf:5f}\")\n",
    "\n",
    "# let us listen to generated samples\n",
    "from IPython.display import display, Audio\n",
    "display(Audio(wav.view(-1).cpu().numpy(), rate=text2speech.fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d3b1d-13e1-4a7c-a1e5-75a33609fb77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
